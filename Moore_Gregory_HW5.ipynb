{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CHEM E 545 Homework 5 (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Classification using SVM (25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Image Classification with SVM**\n",
    "\n",
    "Problem Statement:\n",
    "In this assignment, you are tasked with performing image classification on a dataset containing two classes: 'forest' and 'ocean'. The dataset is divided into 'train' and 'test' folders, each containing images. Your goal is to build a Support Vector Machine (SVM) model with hyperparameter optimization using cross validation. Additionally, you will analyze the accuracy and computational efficiency of the algorithm.\n",
    "\n",
    "[Link to the Data](https://drive.google.com/drive/folders/15fjBRD7pDRiha7fDez_PX7hWfUoRZWkp?usp=sharing)\n",
    "\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Data Preparation (10):\n",
    "   - Read the images from the 'train' and 'test' folders.\n",
    "   - Resize the images to a fixed size of 16x16 pixels with 3 color channels (RGB). Flatten the image to make a vector of size 768.\n",
    "   - Normalize the pixel values of the images to the range [0, 1] to ensure consistent data for model training.\n",
    "\n",
    "2. Model Training and Optimization (10):\n",
    "   - Train the model and use cross validation to optimize the hyperparameters. Try both the linear and rbf kernel types. \n",
    "   - Explain your results based on your intuition\n",
    "\n",
    "3. Model Evaluation (3):\n",
    "   - Train the final SVM model (using the best parameters from above) using the entire training dataset.\n",
    "   - Report both the train and test accuracy.\n",
    "\n",
    "\n",
    "4. Computational Efficiency Analysis (2):\n",
    "   - Measure and report the time taken for model training and testing for each SVM model.\n",
    "   - Analyze and compare the computational efficiency of the linear and Gaussian kernel SVM models. Consider the training and prediction times.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After unzipping, I commented out the two unzip lines and reran the code so that the terminal output is hidden\n"
     ]
    }
   ],
   "source": [
    "# you can unzip the data using the code below\n",
    "\n",
    "#!unzip test.zip\n",
    "#!unzip train.zip\n",
    "\n",
    "print('After unzipping, I commented out the two unzip lines and reran the code so that the terminal output is hidden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 Data Preparation\n",
    "from os import listdir\n",
    "from PIL import Image as PImage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a function to import the images from the folder and do some processing on them\n",
    "def ImageImport(location):\n",
    "    folder = listdir(location) \n",
    "    vector_array = []\n",
    "    for img in folder: #for loop to iterate through each image in the folder, which is passed to the function as a parameter\n",
    "        img = PImage.open(location + img).resize((16,16)).convert('RGB') #define image object and use PImage.open it, .resize() to resize it to 16x16\n",
    "        # and .convert() to convert it to RGB\n",
    "        vector = np.array(img).flatten() #make a 3D array called vector which is the array form of the the image, and then .flatten() to make it a 1D vector of length 768\n",
    "        vector_array.append(vector) #append the vector to vector_array\n",
    "    return vector_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run the ImageImport Function 4 times, each one passed a different directory corresponding to forest/ocean train/test\n",
    "f_train = ImageImport('train/forest/')\n",
    "f_test = ImageImport('test/forest/')\n",
    "o_train = ImageImport('train/ocean/')\n",
    "o_test = ImageImport('test/ocean/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler() #From preprocessing use MinMaxScaler to scale all the data between [0,1]\n",
    "f_train_sc = min_max_scaler.fit_transform(f_train) # for the train data; fit and transform, for the test data; only transform\n",
    "f_test_sc = min_max_scaler.fit(f_test)\n",
    "o_train_sc = min_max_scaler.fit_transform(o_train)\n",
    "o_test_sc = min_max_scaler.fit(o_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#0 = ocean, 1 = forest\n",
    "#Convert the o_train array into a dataframe\n",
    "o_train_df = pd.DataFrame(o_train_sc) \n",
    "#and add a column called 'clf' (classification) with all the values being 0\n",
    "o_train_df['clf'] = 0 \n",
    "f_train_df = pd.DataFrame(f_train_sc) #do the same thing for the forest dataframe\n",
    "f_train_df['clf'] = 1 #with all classifications being 1 \n",
    "#concat the two dataframes atop each other\n",
    "train = pd.concat([o_train_df,f_train_df], ignore_index=True) \n",
    "X_train = train.drop(['clf'], axis=1) #X_train is all of the scaled RGB vectors\n",
    "Y_train = train['clf'] #Y_train is classification of what each image is \n",
    "\n",
    "#repeat the process for the ocean test and ocean train data\n",
    "o_test_df = pd.DataFrame(o_test)\n",
    "o_test_df['clf'] = 0\n",
    "f_test_df = pd.DataFrame(f_test)\n",
    "f_test_df['clf'] = 1\n",
    "test = pd.concat([o_test_df,f_test_df], ignore_index=True)\n",
    "X_test = test.drop(['clf'], axis=1)\n",
    "Y_test = test['clf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Set 1.0\n",
      "Accuracy on Testing Set 0.645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "#First try regular linear model before cross-validation and tuning\n",
    "clf_svm = LinearSVC(max_iter=10000, random_state=42).fit(X_train, Y_train)\n",
    "print('Accuracy on Training Set ' + str(clf_svm.score(X_train,Y_train)))\n",
    "print('Accuracy on Testing Set ' + str(clf_svm.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "The best model was: LinearSVC(C=4.111111111111111, max_iter=10000)\n",
      "The best parameter values were: {'C': 4.111111111111111}\n",
      "The best f1-score was: 0.9831123234108808\n"
     ]
    }
   ],
   "source": [
    "#Training the model and optimizing the hyperparemeters: Linear Kernel\n",
    "#parameters dictionary to find the optimal C parameter, using an np.linspace array with 10 values between 0.01 and 20 \n",
    "#On first pass I tried 10 values between 0.01 and 20, and it returned C=4.45\n",
    "#so I am trying again with a narrower range of values to hone in \n",
    "parameters_dict = { 'C': np.linspace(3,5,num=10)}\n",
    "svc = LinearSVC(max_iter = 10000)\n",
    "#initializing gridsearch object\n",
    "grid_search = GridSearchCV(svc, parameters_dict, scoring='f1',\n",
    "                           return_train_score=True, cv=5, verbose=1) \n",
    "#fitting the grid search to X_train and y_train\n",
    "grid_search.fit(X_train,Y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_parameters = grid_search.best_params_\n",
    "best_f1 = grid_search.best_score_\n",
    "\n",
    "print('The best model was:', best_model)\n",
    "print('The best parameter values were:', best_parameters)\n",
    "print('The best f1-score was:', best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "The best model was: SVC(C=20.0, gamma=0.01)\n",
      "The best parameter values were : {'C': 20.0, 'gamma': 0.01}\n",
      "The best f1-score was: 0.9730046751168778\n"
     ]
    }
   ],
   "source": [
    "#RBF Kernel SVC\n",
    "from sklearn.svm import SVC\n",
    "parameters_dict_rbf = {'C': np.linspace(0.01,20,num=10),\n",
    "                      'gamma': np.linspace(0.01,0.1,num=5)}\n",
    "svc_rbf = SVC(kernel='rbf')\n",
    "grid_search_rbf = GridSearchCV(svc_rbf, parameters_dict_rbf,\n",
    "                               scoring='f1', return_train_score = True,\n",
    "                               cv=5, verbose=1)\n",
    "grid_search_rbf.fit(X_train,Y_train)\n",
    "print('The best model was: ' + str(grid_search_rbf.best_estimator_))\n",
    "print('The best parameter values were : ' + str(grid_search_rbf.best_params_))\n",
    "print('The best f1-score was: ' + str(grid_search_rbf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that a larger C value (as opposed to small ones like 0.1) is favorable for this data which indicates a harder margin is preferred. Since a small gamma value was found from GridSearchCV it means each individual training example has a large influence, hence the decision boundary becomes less linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Training time = 2.547 sec\n",
      "Linear SVC Accuracy on Training Set 1.0\n",
      "Linear SVC Accuracy on Testing Set 0.6475\n",
      "Linear SVC Testing time =  0.013 sec\n",
      "RBF Training Time 0.306 sec\n",
      "RBF Accuracy on Training Set 1.0\n",
      "RBF Accuracy on Testing Set 0.5\n",
      "RBF Testing time =  0.094 sec\n"
     ]
    }
   ],
   "source": [
    "#Train the best model from above using the entire training data\n",
    "#and report the train and test accuracy\n",
    "#Linear Model\n",
    "import time #Use the time package to measure the time taken to train the model\n",
    "svm_start = time.time()\n",
    "svm_linear = LinearSVC(max_iter = 10000, C = 4.11,random_state=42).fit(X_train,Y_train)\n",
    "svm_stop = time.time()\n",
    "print('Linear SVC Training time = {:.3f} ' .format(svm_stop - svm_start) + 'sec')\n",
    "print('Linear SVC Accuracy on Training Set ' + str(svm_linear.score(X_train,Y_train)))\n",
    "test_start = time.time()\n",
    "print('Linear SVC Accuracy on Testing Set ' + str(svm_linear.score(X_test, Y_test)))\n",
    "test_stop = time.time()\n",
    "print('Linear SVC Testing time =  {:.3f} ' .format(test_stop-test_start) + 'sec')\n",
    "\n",
    "#RBF Kernel\n",
    "rbf_start = time.time()\n",
    "svm = SVC(kernel='rbf', C=20, gamma=0.01).fit(X_train,Y_train)\n",
    "rbf_stop = time.time()\n",
    "print('RBF Training Time {:.3f} ' .format(rbf_stop - rbf_start) + 'sec')\n",
    "print('RBF Accuracy on Training Set ' + str(svm.score(X_train,Y_train)))\n",
    "test_start = time.time()\n",
    "print('RBF Accuracy on Testing Set ' + str(svm.score(X_test, Y_test)))\n",
    "test_stop = time.time()\n",
    "print('RBF Testing time =  {:.3f} ' .format(test_stop-test_start) + 'sec')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the gaussian radial basis function took less time to train than the linear support vector model, making it more computationally efficient. However, it performed worse on the testing set than the linear model did; and the linear model was still only 2.547 seconds to train. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Conceptual Questions about SVM (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Match the following classifers (given in the figure below) to the descriptions below. Support vectors are given by solid pointers.\n",
    "\n",
    "(a) A linear support vector machine with C = 0.1.\n",
    "\n",
    "(b) A linear support vector machine with C = 10.\n",
    "\n",
    "(c) A  support vector machine with K(u, v) = u · v + .  $(u · v)^{2}$\n",
    "\n",
    "(d) A  support vector machine with K(u, v) = exp ( -$\\frac{1}{4}$  $∥u − v∥^{2}$ ).\n",
    "\n",
    "(e) A  support vector machine with K(u, v) = exp ( -4  $∥u − v∥^{2}$ )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](SVM_examples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) linear svm C = 0.1 - Number 4\n",
    "\n",
    "B) linear svm C = 10 - Number 3\n",
    "\n",
    "C) A support vector machine K(u, v) = u · v + .  (𝑢·𝑣)2 - Number 5\n",
    "\n",
    "D) svm with K(u,v) = exp(-1/4 ||u-v||^2) - Number 6\n",
    "\n",
    "E) svm with K(u,v) = exp(-4 ||u-v||^2) - Number 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
